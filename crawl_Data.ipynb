{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff248ece-b09f-460b-b604-2c4a7fc16813",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 125\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0) # Giúp chạy các hàm của scikit-learn giống nhau mỗi lần chạy\n",
    "\n",
    "import regex as re\n",
    "import time # Dùng để sleep chương trình\n",
    "from tqdm.notebook import tqdm # Hiện thanh progress cho đẹp :D\n",
    "tqdm.pandas()\n",
    "\n",
    "# Thư viện để request và parse HTML\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Các thư viện liên quan tới ngôn ngữ và NLP\n",
    "from pyvi import ViTokenizer # Thư viện NLP tiếng Việt\n",
    "import gensim\n",
    "import unicodedata # Thư viện unicode\n",
    "\n",
    "# Trực quan hóa mô hình dự đoán văn bản\n",
    "from lime import lime_text\n",
    "\n",
    "# Dùng để lưu lại model\n",
    "import pickle\n",
    "\n",
    "# Thư viện liên quan của Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import feature_selection\n",
    "\n",
    "# Tạo pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "\n",
    "# Các mô hình học\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier # Phương pháp bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d7a82b6-efed-4a91-ae66-69718a493937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thiết lập đường dẫn cho phần 1\n",
    "dir_1 = \"C:/Users/davin/Desktop/KHDL/\"\n",
    "# Thiết lập lại đường dẫn phần 1\n",
    "dir_1_new = \"C:/Users/davin/Desktop/KHDL/raw_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b9f0212-56a9-480d-8fcb-4efda49af3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HÀM REQUEST CONTENT\n",
    "def single_request_scraping(index = 1, limit_retry = 100, sleep_time = 0.05):\n",
    "\n",
    "    #THIẾT LẬP YÊU CẦU HTTP\n",
    "    allow_status = [200, 301]\n",
    "    # Tạo một phiên làm việc bằng requests.Session()\n",
    "    s = requests.Session()\n",
    "    #cấu hình User-Agent giả lập trình duyệt, giảm nguy cơ bị chặn từ phía trang web\n",
    "    s.headers['User-Agent'] = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36'\n",
    "    url = f\"https://tuoitre.vn/timeline/0/trang-{index}.htm\"\n",
    "    \n",
    "    \n",
    "    # LẤY DANH SÁCH NEWS_ITEMS\n",
    "    try:\n",
    "    #Gửi yêu cầu GET đến url\n",
    "        response = s.get(url)\n",
    "        try_left = limit_retry\n",
    "        while (response.status_code not in allow_status and try_left > 0):\n",
    "            print(f\"Loi {response.status_code} tai trang {index}\")\n",
    "            response = s.get(url)\n",
    "            try_left -= 1\n",
    "        html_text = response.text\n",
    "    except:\n",
    "        print(f\"Loi request tai trang {index}\")\n",
    "        return None\n",
    "        \n",
    "    \n",
    "    # PHÂN TÍCH HTML VÀ CHUYỂN ĐỔI HTML_TEXT THÀNH ĐỐI TƯỢNG TREE HỖ TRỢ TRUY SUẤT CÁC PHẦN TỬ HTML\n",
    "    html_tree = BeautifulSoup(html_text, 'html.parser')\n",
    "    news_items = html_tree.find_all('div', class_='box-category-item')\n",
    "    \n",
    "    #Kiểm tra lỗi và số item\n",
    "    if (len(news_items) == 0):\n",
    "        print(f'Trang {index} khong lay duoc item.')\n",
    "        print(html_text)\n",
    "        return None\n",
    "    elif (len(news_items) != 20):\n",
    "        print(f\"Warning: Trang {index} chi lay duoc {len(news_items)} items.\")\n",
    "    \n",
    "    \n",
    "    # LẤY CÁC DỮ LIỆU LINK, TITLE, DESCRIPTION, CLASS\n",
    "    # Tạo danh sách để lưu dữ liệu\n",
    "    single_data = []\n",
    "    # Lấy ra link, title, description và category từ mỗi bài báo\n",
    "    for item in news_items:\n",
    "            \n",
    "        # Lấy title và link\n",
    "        link_tag = item.find('a', class_='box-category-link-title')\n",
    "        if link_tag:\n",
    "            title = link_tag.get('title', '').strip()\n",
    "            link = \"https://tuoitre.vn\" + link_tag.get('href', '')\n",
    "        \n",
    "        # Lấy category\n",
    "        category_tag = item.find('a', class_='box-category-category')\n",
    "        category = category_tag.text.strip() if category_tag else \"\"\n",
    "        \n",
    "        # Lấy description\n",
    "        description_tag = item.find('p', class_='box-category-sapo')\n",
    "        description = description_tag.text.strip() if description_tag else \"\"\n",
    "        \n",
    "        # Thêm dữ liệu vào danh sách\n",
    "        single_data.append({\"links\": link, \"title\": title, \"description\": description, \"class\": category})\n",
    "    \n",
    "    raw_data = pd.DataFrame(single_data)\n",
    "    \n",
    "    # LẤY DỮ LIỆU CONTENT\n",
    "    raw_data[\"content\"] = \"\"\n",
    "    \n",
    "    #phân tích HTML\n",
    "    for tmp_index, row in raw_data.iterrows():\n",
    "        try:\n",
    "            response = s.get(row[\"links\"])\n",
    "            try_left = limit_retry\n",
    "            while (response.status_code not in allow_status and try_left > 0):\n",
    "                print(f\"Loi {response.status_code} tai link {row['links']}\")\n",
    "                response = s.get(row[\"links\"])\n",
    "                try_left -= 1\n",
    "            news_page = response.content\n",
    "        except:\n",
    "            print(f\"Loi request tai link {row['links']}\")\n",
    "            \n",
    "    #chuyển đổi html_text thành đối tree giúp truy xuất các phần tử trong HTML\n",
    "        tmp_tree = BeautifulSoup(news_page, \"html.parser\")\n",
    "    \n",
    "    # Lấy nội dung\n",
    "        try:\n",
    "            body = tmp_tree.find(\"div\", class_=\"detail-content afcbc-body\")\n",
    "            content = body.findChildren(\"p\", recursive=False)\n",
    "            tmp_string = \"\"\n",
    "            for x in content:\n",
    "                tmp_string += x.text\n",
    "                raw_data.at[tmp_index, \"content\"] = tmp_string\n",
    "        except:\n",
    "            raw_data.at[tmp_index, \"content\"] = \"\"\n",
    "    # Thời gian nghỉ tránh bị khoá ip\n",
    "        time.sleep(sleep_time)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253a96b6-f3f8-407c-8d9b-2c161c49f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch scraping\n",
    "def batch_scraping(num = 200, output_dir = \"\"):\n",
    "    '''\n",
    "        num: số lượng trang request cho 1 batch (1 trang = 20 bài báo). Mặc định 4000 bài báo.\n",
    "    '''\n",
    "    iter_num = 1  # Số batch bắt đầu\n",
    "    continue_flag = True # Cờ hiệu kết thúc vòng lặp khi xảy ra lỗi\n",
    "\n",
    "    while (continue_flag):\n",
    "        '''\n",
    "        Khởi tạo dataframe rỗng. \n",
    "        Sau đó lấy đủ 1 số trang cho 1 batch.\n",
    "        Rồi export file csv.\n",
    "        '''\n",
    "        \n",
    "        batch_df = pd.DataFrame(columns=[\"links\", \"title\", \"description\", \"class\", \"content\"])\n",
    "        \n",
    "        for index in range(iter_num * num + 1, (iter_num + 1) * num + 1):\n",
    "            data = single_request_scraping(index)\n",
    "            if (data is None):\n",
    "                continue_flag = False\n",
    "                break\n",
    "            print(f\"Page {index} complete!\")\n",
    "            batch_df = pd.concat([batch_df, data], ignore_index=True)\n",
    "        batch_df.to_csv(output_dir + f'crawling_{iter_num}.csv',index=False,encoding=\"utf-8\")\n",
    "        iter_num+=1\n",
    "        \n",
    "batch_scraping(output_dir = dir_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a3904a-a2de-4f80-b5bd-a1e9d0f7346c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
